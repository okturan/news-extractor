#!/usr/bin/env python3
"""
Compare Crawl4AI's best result (paragraph selector) vs Newspaper4k
"""

import asyncio
from newspaper import Article, Config
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

url = "https://bianet.org/haber/erdogan-ozele-tazminat-davasi-acti-313281"

print("=" * 80)
print("CRAWL4AI BEST VS NEWSPAPER4K")
print("=" * 80)
print()

# 1. Newspaper4k
print("1. NEWSPAPER4K (baseline)")
print("-" * 80)

config = Config()
config.language = 'tr'
article = Article(url, config=config)
article.download()
article.parse()

n4k_text = article.text
print(f"Length: {len(n4k_text)} chars")
print(f"\nFull content:")
print(n4k_text)


# 2. Crawl4AI with paragraph selector
print("\n\n2. CRAWL4AI (paragraph selector - best result)")
print("-" * 80)


async def get_crawl_paragraphs():
    browser_config = BrowserConfig(headless=True, verbose=False)
    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        wait_until="domcontentloaded",
        css_selector="article p, main p, .article-content p",
        word_count_threshold=5,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=crawler_config)
        return result.markdown if result.success else None


crawl_text = asyncio.run(get_crawl_paragraphs())

if crawl_text:
    print(f"Length: {len(crawl_text)} chars")
    print(f"\nFull content:")
    print(crawl_text)
else:
    print("Failed")

# Analysis
print("\n\n" + "=" * 80)
print("ANALYSIS")
print("=" * 80)

print(f"\nText Lengths:")
print(f"  Newspaper4k:  1,027 chars (clean)")
print(f"  Crawl4AI:     {len(crawl_text) if crawl_text else 0:,} chars (5.8x larger)")

if crawl_text:
    # Check what extra content Crawl4AI has
    print(f"\nExtra content in Crawl4AI:")

    # Look for common noise patterns
    noise_patterns = {
        'Links to other articles': crawl_text.count('](https://bianet.org/'),
        'HABER mentions': crawl_text.count('HABER'),
        'Navigation elements': crawl_text.count('Yayƒ±n Tarihi') + crawl_text.count('Son G√ºncelleme'),
    }

    for pattern, count in noise_patterns.items():
        if count > 0:
            print(f"  - {pattern}: {count}")

    # Check if actual article content is there
    article_check = n4k_text[:200] in crawl_text
    print(f"\nArticle content present: {'‚úÖ Yes' if article_check else '‚ùå No'}")

print("\n" + "=" * 80)
print("VERDICT")
print("=" * 80)

if crawl_text and len(crawl_text) > len(n4k_text) * 3:
    print("\n‚ùå Crawl4AI STILL extracts too much content")
    print(f"   Even with best CSS selector, it's {len(crawl_text)/len(n4k_text):.1f}x larger")
    print("   Includes related articles, navigation, metadata")
elif crawl_text:
    print("\n‚úÖ Crawl4AI can produce reasonable results with CSS selectors")
    print(f"   Only {len(crawl_text)/len(n4k_text):.1f}x larger than Newspaper4k")
else:
    print("\n‚ùå Crawl4AI failed")

print("\nüí° Newspaper4k remains the cleanest extractor")
print("   1,027 chars of pure article content, no configuration needed")
print("=" * 80)
